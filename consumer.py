from kafka import KafkaConsumer
import json
import os
import subprocess

# VÃ©rifier si le fichier existe sur la machine locale
if not os.path.exists("pollution_data.txt"):
    open("pollution_data.txt", "w").close()

# Connexion au topic "pollution"
consumer = KafkaConsumer(
    "pollution",
    bootstrap_servers="localhost:9092",
    auto_offset_reset="earliest",
    group_id="pollution-group",
    value_deserializer=lambda x: json.loads(x.decode("utf-8"))
)

print("ğŸ“¥ En attente des donnÃ©es Kafka...")

for message in consumer:
    data = message.value  # RÃ©cupÃ©rer les donnÃ©es envoyÃ©es par Kafka
    print("ğŸ“¥ DonnÃ©es reÃ§ues depuis Kafka :", data)  # Debug

    # VÃ©rifier si la clÃ© 'parameter' existe dans le message
    if "parameter" in data and "value" in data and "period" in data:
        formatted_data = f"""
        ğŸ”¬ ParamÃ¨tre: {data['parameter']['name']}
        ğŸ“Š UnitÃ©: {data['value']} {data['parameter']['units']}
        ğŸ“… PremiÃ¨re mesure: {data['period']['datetimeFrom']['local']}
        ğŸ“… DerniÃ¨re mesure: {data['period']['datetimeTo']['local']}
        """

        print(formatted_data)

        # Ã‰crire dans le fichier local
        with open("pollution_data.txt", "a", encoding="utf-8") as file:
            file.write(formatted_data + "\n")
            file.flush()

        # Copie dans Docker puis envoi dans HDFS
        if os.path.exists("pollution_data.txt"):
            print("ğŸ“‚ Copie du fichier dans Docker...")
            subprocess.run(["docker", "cp", "pollution_data.txt", "hadoop-master:/tmp/pollution_data.txt"])

            print("ğŸš€ Envoi du fichier dans HDFS...")
            subprocess.run([
                "docker", "exec", "-it", "hadoop-master",
                "hdfs", "dfs", "-appendToFile", "/tmp/pollution_data.txt", "/data/pollution_data.txt"
            ])
    else:
        print("âš ï¸ DonnÃ©es non exploitables reÃ§ues :", data)
