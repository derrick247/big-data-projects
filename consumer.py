from kafka import KafkaConsumer
import json
import os
import subprocess
import time  # Pour gÃ©nÃ©rer un group_id unique

# VÃ©rifier si le fichier existe sur la machine locale
if not os.path.exists("pollution_data.txt"):
    open("pollution_data.txt", "w").close()

# GÃ©nÃ©rer un `group_id` unique pour chaque exÃ©cution
unique_group_id = f"pollution-group-{int(time.time())}"

# Connexion au topic "pollution"
consumer = KafkaConsumer(
    "pollution",
    bootstrap_servers="localhost:9092",
    auto_offset_reset="latest",  # Lire SEULEMENT les nouveaux messages
    group_id=unique_group_id,  # Nouveau group_id unique Ã  chaque exÃ©cution
    enable_auto_commit=True,  # Sauvegarde automatique de la position de lecture
    value_deserializer=lambda x: json.loads(x.decode("utf-8"))
)

print(f"ğŸ“¥ En attente des nouvelles donnÃ©es Kafka... (Group ID: {unique_group_id})")

for message in consumer:
    data = message.value  # RÃ©cupÃ©rer les donnÃ©es envoyÃ©es par Kafka
    print("ğŸ“¥ Nouvelle donnÃ©e reÃ§ue :", data)

    # VÃ©rifier si la clÃ© 'parameter' existe dans le message
    if "parameter" in data and "value" in data and "period" in data:
        formatted_data = f"""
        ğŸ”¬ ParamÃ¨tre: {data['parameter']['name']}
        ğŸ“Š UnitÃ©: {data['value']} {data['parameter']['units']}
        ğŸ“… PremiÃ¨re mesure: {data['period']['datetimeFrom']['local']}
        ğŸ“… DerniÃ¨re mesure: {data['period']['datetimeTo']['local']}
        """

        print(formatted_data)

        # Ã‰crire dans le fichier local
        with open("pollution_data.txt", "a", encoding="utf-8") as file:
            file.write(formatted_data + "\n")
            file.flush()

        # Copie dans Docker puis envoi dans HDFS
        if os.path.exists("pollution_data.txt"):
            print("ğŸ“‚ Copie du fichier dans Docker...")
            subprocess.run(["docker", "cp", "pollution_data.txt", "hadoop-master:/tmp/pollution_data.txt"])

            print("ğŸš€ Envoi du fichier dans HDFS...")
            subprocess.run([
                "docker", "exec", "-it", "hadoop-master",
                "hdfs", "dfs", "-appendToFile", "/tmp/pollution_data.txt", "/data/pollution_data.txt"
            ])
    else:
        print("âš ï¸ DonnÃ©es non exploitables reÃ§ues :", data)
