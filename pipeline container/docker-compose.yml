services:
    kafka:
        image: apache/kafka:latest
        ports:
            - 9092:9092

    namenode:
        image: apache/hadoop:3.3.5
        container_name: namenode
        hostname: namenode
        user: root
        environment:
            - HADOOP_HOME=/opt/hadoop
        volumes:
            - ./hadoop_namenode:/opt/hadoop/data/nameNode
            - ./hadoop_config:/opt/hadoop/etc/hadoop
            - ./start-hdfs.sh:/start-hdfs.sh
        ports:
            - "9870:9870"
            - "9000:9000"
        command: ["/bin/bash", "/start-hdfs.sh"]
        # networks:
        #   hdfs_network:
        #     ipv4_address: 172.20.0.2

    datanode1:
        image: apache/hadoop:3.3.5
        container_name: datanode1
        hostname: datanode1
        user: root
        environment:
            - HADOOP_HOME=/opt/hadoop
        volumes:
            - ./hadoop_datanode1:/opt/hadoop/data/dataNode
            - ./hadoop_config:/opt/hadoop/etc/hadoop
            - ./init-datanode.sh:/init-datanode.sh
        depends_on:
            - namenode
        command: ["/bin/bash", "/init-datanode.sh"]
        ports:
            - 9864:9864
        # networks:
        #   hdfs_network:
        #     ipv4_address: 172.20.0.3

    # kafka-producer:
    #   build: ./producer/.
    #   container_name: kafka-producer
    #   depends_on:
    #     - kafka
    #   environment:
    #     KAFKA_BROKER: kafka:9092
    #   restart: always

networks:
    kafka-logstash:
        external: true
